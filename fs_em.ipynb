{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-auth-oauthlib google-auth-httplib2 google-api-python-client tldextract requests aiohttp beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tZKLIr_b9zFi",
        "outputId": "e76a2550-683c-49b6-caf6-05139e7efb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.166.0)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (5.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.38.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.2.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-auth-oauthlib) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPNqDqSG-yLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5gf4sWf9HZq",
        "outputId": "e78ad658-daec-47ce-a077-3893ae2bf021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Sheet: Videographers\n"
          ]
        }
      ],
      "source": [
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import requests\n",
        "import re\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import unquote, urljoin\n",
        "import tldextract\n",
        "\n",
        "# Extended configuration from app.py\n",
        "IGNORE_DOMAINS = [\n",
        "    \"wix.com\", \"mysite.com\", \"domain.com\", \"example.com\",\n",
        "    \"sentry.io\", \"wixpress.com\", \"squarespace.com\",\n",
        "    \"wordpress.com\", \"email.com\", \"shopify.com\"\n",
        "]\n",
        "\n",
        "COMMON_EMAIL_DOMAINS = [\n",
        "    \"gmail.com\", \"yahoo.com\", \"outlook.com\", \"hotmail.com\",\n",
        "    \"aol.com\", \"icloud.com\", \"protonmail.com\", \"mail.com\",\n",
        "    \"zoho.com\", \"yandex.com\", \"gmx.com\"\n",
        "]\n",
        "\n",
        "CONTACT_PAGES = [\n",
        "    \"/contact\", \"/contact-us\", \"/contact.html\", \"/contact-us.html\",\n",
        "    \"/about\", \"/about-us\", \"/about.html\", \"/about-us.html\",\n",
        "    \"/get-in-touch\", \"/reach-us\", \"/connect\", \"/reach-out\",\n",
        "    \"/our-team\", \"/team\", \"/support\", \"/help\", \"/info\"\n",
        "]\n",
        "\n",
        "def validate_email(email):\n",
        "    \"\"\"Validate and clean email addresses\"\"\"\n",
        "    email = email.strip().lower()\n",
        "\n",
        "    # Ignore image files and other non-email strings\n",
        "    if any(ext in email for ext in [\".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".webp\", \".ico\"]):\n",
        "        return None\n",
        "\n",
        "    # Remove invalid start/end characters\n",
        "    email = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9\\.]+$', '', email)\n",
        "\n",
        "    # Check email pattern and domain structure\n",
        "    if re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', email):\n",
        "        parts = email.split('@')\n",
        "        if len(parts) == 2 and \".\" in parts[1]:\n",
        "            domain_part = parts[1]\n",
        "            if not re.search(r'\\d+x\\d+', domain_part):\n",
        "                return email\n",
        "    return None\n",
        "\n",
        "def get_domain(url):\n",
        "    \"\"\"Extract domain from URL\"\"\"\n",
        "    try:\n",
        "        extracted = tldextract.extract(url)\n",
        "        return f\"{extracted.domain}.{extracted.suffix}\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def clean_and_deduplicate_emails(emails_list):\n",
        "    \"\"\"Clean and deduplicate email list\"\"\"\n",
        "    if not emails_list:\n",
        "        return []\n",
        "\n",
        "    clean_emails = set()\n",
        "    for email in emails_list:\n",
        "        valid_email = validate_email(email)\n",
        "        if valid_email and not any(ignore_domain in valid_email for ignore_domain in IGNORE_DOMAINS):\n",
        "            clean_emails.add(valid_email)\n",
        "\n",
        "    # Similar deduplication logic from app.py\n",
        "    emails_to_remove = set()\n",
        "    final_emails = list(clean_emails)\n",
        "\n",
        "    for i in range(len(final_emails)):\n",
        "        for j in range(len(final_emails)):\n",
        "            if i != j and final_emails[i] != final_emails[j]:\n",
        "                email1_parts = final_emails[i].split('@')\n",
        "                email2_parts = final_emails[j].split('@')\n",
        "\n",
        "                if len(email1_parts) == 2 and len(email2_parts) == 2 and email1_parts[1] == email2_parts[1]:\n",
        "                    username1, username2 = email1_parts[0], email2_parts[0]\n",
        "\n",
        "                    if username1 in username2:\n",
        "                        emails_to_remove.add(final_emails[j])\n",
        "                    elif username2 in username1:\n",
        "                        emails_to_remove.add(final_emails[i])\n",
        "\n",
        "    final_cleaned = [email for email in final_emails if email not in emails_to_remove]\n",
        "    return final_cleaned\n",
        "\n",
        "async def fetch_emails(session, url):\n",
        "    \"\"\"Enhanced email extraction method\"\"\"\n",
        "    try:\n",
        "        # Ensure URL has proper prefix\n",
        "        if not url.startswith(('http://', 'https://')):\n",
        "            url = 'https://' + url\n",
        "\n",
        "        domain = get_domain(url)\n",
        "        emails_set = set()\n",
        "\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "        async with session.get(url, headers=headers, timeout=15) as response:\n",
        "            if response.status == 200:\n",
        "                text = await response.text()\n",
        "                soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "                # Multiple email extraction methods\n",
        "                text_emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
        "\n",
        "                # Mailto links\n",
        "                mailto_emails = [unquote(a['href'].replace('mailto:', '').split('?')[0])\n",
        "                                 for a in soup.find_all('a', href=True) if 'mailto:' in a['href']]\n",
        "\n",
        "                # Script and meta tag emails\n",
        "                script_emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
        "                                           ' '.join([script.string for script in soup.find_all('script') if script.string]))\n",
        "\n",
        "                meta_emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n",
        "                                         ' '.join([meta.get('content', '') for meta in soup.find_all('meta')]))\n",
        "\n",
        "                # Domain-based email extraction\n",
        "                domain_emails = [email for email in (text_emails + mailto_emails + script_emails + meta_emails)\n",
        "                                 if domain and domain in email]\n",
        "\n",
        "                all_emails = text_emails + mailto_emails + script_emails + meta_emails\n",
        "\n",
        "                # Clean and validate emails\n",
        "                cleaned_emails = clean_and_deduplicate_emails(all_emails)\n",
        "\n",
        "                # Prioritize domain-matched emails\n",
        "                domain_matched_emails = [email for email in cleaned_emails if domain and domain in email]\n",
        "                other_emails = [email for email in cleaned_emails if email not in domain_matched_emails]\n",
        "\n",
        "                emails_set.update(domain_matched_emails + other_emails)\n",
        "\n",
        "                return list(emails_set)[:5]  # Limit to top 5 emails\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting emails from {url}: {e}\")\n",
        "\n",
        "    return [\"No Email Found\"]\n",
        "\n",
        "# Rest of the script remains the same as your previous implementation\n",
        "async def process_websites(websites):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch_emails(session, url) for url in websites]\n",
        "        return await asyncio.gather(*tasks)\n",
        "\n",
        "def run_async_in_process(websites):\n",
        "    return asyncio.run(process_websites(websites))\n",
        "\n",
        "def process_sheet(spreadsheet_url):\n",
        "    # Your existing sheet processing logic remains the same\n",
        "    sh = gc.open_by_url(spreadsheet_url)\n",
        "    for worksheet in sh.worksheets():\n",
        "        print(f\"Processing Sheet: {worksheet.title}\")\n",
        "        data = worksheet.get_all_values()\n",
        "        if not data:\n",
        "            continue\n",
        "\n",
        "        headers = data[0]\n",
        "        website_col_idx = next((i for i, col in enumerate(headers) if \"website\" in col.lower()), None)\n",
        "\n",
        "        if website_col_idx is None:\n",
        "            print(\"No website column found in sheet.\")\n",
        "            continue\n",
        "\n",
        "        headers = data[0]\n",
        "        data_rows = data[1:]\n",
        "\n",
        "        websites = [row[website_col_idx] for row in data[1:] if row[website_col_idx]]\n",
        "\n",
        "        # Use multiprocessing to run the async part in a separate process\n",
        "        with multiprocessing.Pool(1) as pool:\n",
        "            emails_list = pool.apply(run_async_in_process, args=(websites,))\n",
        "\n",
        "        rows_to_delete = []\n",
        "\n",
        "        # Process emails and update sheet\n",
        "        for i, (row, emails) in enumerate(zip(data_rows, emails_list), start=1):\n",
        "            if emails == [\"No Email Found\"]:\n",
        "                # Mark this row for deletion\n",
        "                rows_to_delete.append(i + 1)  # +1 to account for header row\n",
        "            else:\n",
        "                # Update emails in the next column\n",
        "                for j, email in enumerate(emails):\n",
        "                    worksheet.update_cell(i + 1 + j, website_col_idx + 2, email)\n",
        "\n",
        "        # Delete rows with no emails (in reverse order to maintain correct indexing)\n",
        "        if rows_to_delete:\n",
        "            for row in sorted(rows_to_delete, reverse=True):\n",
        "                worksheet.delete_rows(row)\n",
        "            print(f\"Deleted {len(rows_to_delete)} rows with no emails found.\")\n",
        "\n",
        "# Authenticate and run\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Run the script (Replace with your Google Sheet URL)\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/162BOhjt8OK274CTfzmRRPqyyBuA787BSZ88dj5kunSg/edit?gid=0#gid=0\"\n",
        "process_sheet(spreadsheet_url)\n",
        "\n",
        "print(\"✅ Done! Emails have been extracted and added to your sheet.\")"
      ]
    }
  ]
}